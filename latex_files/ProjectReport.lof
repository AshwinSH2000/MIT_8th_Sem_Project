\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Standard blocks of Autonomous Driving System \cite {kiran2021deep} }}{2}{figure.1.1}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces Classification of Machine Learning algorithms \cite {peng2021machine} }}{3}{figure.1.2}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces General scenario of Reinforcement Learning \cite {naveen2020survey} }}{4}{figure.1.3}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Deep Deterministic Policy Gradient block diagram }}{13}{figure.4.1}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Driving Environment \cite {huang2019end} }}{16}{figure.4.2}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces TORCS main screen}}{23}{figure.5.1}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Track selection screen}}{24}{figure.5.2}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Agent driving on the track}}{24}{figure.5.3}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Agent goes off-track}}{25}{figure.5.4}%
\contentsline {figure}{\numberline {5.5}{\ignorespaces Agent slowly getting back on track}}{25}{figure.5.5}%
\contentsline {figure}{\numberline {5.6}{\ignorespaces Training of the agent}}{26}{figure.5.6}%
\contentsline {figure}{\numberline {5.7}{\ignorespaces Rewards vs speed, angle, and position (track 1)}}{27}{figure.5.7}%
\contentsline {figure}{\numberline {5.8}{\ignorespaces Rewards vs speed, angle, and position (track 2)}}{28}{figure.5.8}%
\contentsline {figure}{\numberline {5.9}{\ignorespaces Rewards vs speed, angle, and position (track 3)}}{28}{figure.5.9}%
\contentsline {figure}{\numberline {5.10}{\ignorespaces Rewards vs speed, angle, and position (track 4)}}{29}{figure.5.10}%
\addvspace {10\p@ }
